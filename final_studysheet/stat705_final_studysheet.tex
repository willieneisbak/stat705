\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.2in]{geometry}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1.5ex}{1ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\usepackage{amsmath,amsthm,amssymb,MnSymbol}
\setlength{\parindent}{0in}
\renewenvironment{proof}{{\bfseries Proof}}{\\}
\newcommand{\newlinetab}[0]{$\text{ }\hspace{3mm}$}

%------------------------------------------------------
% Intermediate Statistics 705 Studysheet for Final Test 
%------------------------------------------------------

\begin{document}

\subsection*{Random Variables}
A \textbf{random variable} X is a map $X:\Omega \rightarrow \mathbb{R}$. For $A \subset \mathbb{R}$ we write
\begin{equation}
\mathbb{P}(X \in A) = \mathbb{P}(\{ w \in \Omega : X(w) \in A \})
\end{equation}
The \textbf{cdf} $F_{X}$ of X is
\begin{equation}
F_{X}(x) = \mathbb{P}(X \leq x)
\end{equation}
For continuous X, the \textbf{pdf} $f_{X}$ is a function satisfying
\begin{equation}
\int_{A} f_{X}(x)dx = \mathbb{P}(X \in A)
\end{equation}
Note that $f_{X} = F_{X}'$.

\subsection*{Transformations}
Let $Y=g(X)$, $\mathcal{X} = \{x : f_{X}(x) > 0\}$, and $\mathcal{Y} = \{y : y=g(x), x \in \mathcal{X}\}$ \\($\mathcal{X}$ and $\mathcal{Y}$ called the \emph{support} of X and Y). Then $\forall$ $A \subset \mathcal{Y}$
\begin{equation}
\mathbb{P}(Y \in A) = \mathbb{P}(X \in \{x : g(x) \in A\})
\end{equation}
For the cdf $F_{Y}$
\begin{equation}
F_{Y}(y) = \mathbb{P}(X \in \{x : g(x) \leq y \}) = \int_{\{x : g(x) \leq y \}}f_{X}(x)dx
\end{equation}
For g monotonic
\begin{equation}
F_{Y}(y) =
\begin{cases}
F_{X}(g^{-1}(y))& \text{if $g$ increasing} \\
1 - F_{X}(g^{-1}(y))& \text{if $g$ decreasing}
\end{cases}
\end{equation}
Additionally, for g monotonic, if $g^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$
\begin{equation}
f_{Y}(y) = f_{X}(g^{-1}(y)) \left | \frac{dg^{-1}(y)}{dy} \right | \hspace{5mm} \text{for $y \in \mathcal{Y}$}
\end{equation}

\subsection*{Expected Values}
The \textbf{mean} or \textbf{expected value} of $g(X)$ is
\begin{equation}
\mathbb{E}(g(X)) = \int g(x)dF(x) = \int g(x)dP(x)
\end{equation}
Related properties and definitions:
\begin{flalign}
(a)& \hspace{2mm} \mu = \mathbb{E}(X) \\
(b)& \hspace{2mm} \mathbb{E}(\sum_{i} c_{i}g_{i}(X_{i})) = \sum_{i} c_{i} \mathbb{E}(g_{i}(X_{i})) \\
(c)& \hspace{2mm} \mathbb{E}\left(\prod_{i} X_{i} \right) = \prod_{i} \mathbb{E}(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
(d)& \hspace{2mm} Var(X) = \sigma^{2} = \mathbb{E}((X-\mu)^{2}) \hspace{4mm} \text{is the \textbf{variance} of X} \\
(e)& \hspace{2mm} Var(X) = \mathbb{E}(X^{2}) - \mu^{2} \\
(f)& \hspace{2mm} Var \left (\sum_{i} a_{i}X_{i} \right ) = \sum_{i} a_{i}^{2} Var(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
(g)& \hspace{2mm} Cov(X,Y) = \mathbb{E}((X-\mu_{X})(Y-\mu_{Y})) \hspace{2mm} \text{is the \textbf{covariance}} \\
(h)& \hspace{2mm} Cov(X,Y) = \mathbb{E}(XY) - \mu_{x}\mu_{Y} \\
(i)& \hspace{2mm} \rho(X,Y) = Cov(X,Y) / \sigma_{x}\sigma_{y}, \hspace{4mm} -1 \leq \rho(X,Y) \leq 1
\end{flalign}
The \textbf{conditional expectation} of Y given X is the random variable $g(X) = \mathbb{E}(Y|X)$, where
\begin{gather}
\mathbb{E}(Y|X=x) = \int y f(y|x)dy\\
\text{and} \hspace{2mm} f(y|x) = f_{X,Y}(x,y) / f_{X}(x)
\end{gather}
The \emph{Law of Total/Iterated Expectation} is
\begin{equation}
\mathbb{E}(Y) = \mathbb{E}[\mathbb{E}(Y|X)]
\end{equation}
The \emph{Law of Total Variance} is
\begin{equation}
Var(Y) = Var[\mathbb{E}(Y|X)] + \mathbb{E}[Var(Y|X)]
\end{equation}
The \emph{Law of Total Covariance} is
\begin{equation}
Cov(X,Y) = \mathbb{E}(Cov(X,Y|Z)) + Cov(\mathbb{E}(X|Z), \mathbb{E}(Y|Z))
\end{equation}

\subsection*{Moment Generating Function}
The \textbf{mgf} of X is
\begin{equation}
M_{X}(t) = \mathbb{E}(e^{tX})
\end{equation}
Properties:
\begin{flalign}
(a)& \hspace{2mm} M_{X}^{(n)}(t)|_{t=0} = \mathbb{E}(X^{n}) \hspace{3mm} \text{is the \textbf{$\text{n}^{\text{th}}$ moment} of X} \\
(b)& \hspace{2mm} M_{x}(t) = M_{Y}(t) \hspace{2mm} \forall t \hspace{2mm} \text{around 0} \implies X\stackrel{d}{=}Y \\
(c)& \hspace{2mm} M_{aX+b}(t) = e^{bt}M_{X}(at) \\
(d)& \hspace{2mm} M_{\sum_{i} X_{i}}(t) = \prod_{i} M_{X_{i}}, \hspace{2mm} \text{$X_{1},\ldots,X_{n}$ indep't}
\end{flalign}

\subsection*{Independence}
Random variables X and Y are \textbf{independent}, written $X \upmodels Y$, iff
\begin{equation}
\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B)
\end{equation}
If (X,Y) is a random vector with pdf $f_{X,Y}$, then
\begin{equation}
X \upmodels Y \iff f_{X,Y}(x,y) = f_{X}(x) f_{Y}(y)
\end{equation}

\subsection*{Distributions}
Some discrete distributions:
\begin{flalign}
    \begin{split}
        (a)& \hspace{2mm} \text{Bernoulli} \hspace{2mm} f(x|p) = p^{x}(1-p)^{1-x}, \hspace{3mm} x \in \{0, 1\} \\
            & \hspace{6mm} \text{Mean}=p, \text{Var}=p(1-p)
    \end{split}\\
    \begin{split}
        (b)& \hspace{2mm} \text{Binomial} \hspace{2mm} f(x|n,p) = {\binom{n}{x}} p^{x}(1-p)^{n-x}, \hspace{2mm} x \in \{0,1,\ldots,n\} \\
            & \hspace{6mm} \text{Mean}=np, \text{Var}=np(1-p)
    \end{split}\\
    \begin{split}
        (c)& \hspace{2mm} \text{Poisson} \hspace{2mm} f(x|\lambda) = \frac{e^{-\lambda}\lambda^{x}}{x!}, \hspace{3mm} x \in \{0, 1, 2, \ldots\} \\
            & \hspace{6mm} \text{Mean}=\lambda, \text{Var}=\lambda
    \end{split}
\end{flalign}

Some continuous distributions:
\begin{flalign}
    \begin{split}
        (a)& \hspace{2mm} \text{Uniform} \hspace{2mm} f(x|a,b) = \frac{1}{b-a}, \hspace{3mm} x \in [a,b] \\
            & \hspace{6mm} \text{Mean}=(b+a)/2, \text{Var}=(b-a)^{2}/12
    \end{split}\\
    \begin{split}
        (b)& \hspace{2mm} \text{Normal} \hspace{2mm} f(x|\mu,\sigma^{2}) = \frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^{2}/(2\sigma^{2})}, \hspace{2mm} x \in \mathbb{R} \\
            & \hspace{6mm} \text{Mean}=\mu, \text{Var}=\sigma^{2}
    \end{split}\\
    \begin{split}
        (c)& \hspace{2mm} \text{Gamma} \hspace{2mm} f(x|\alpha,\beta) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}, \hspace{2mm} x \in \mathbb{R}_{+}, \alpha \hspace{1mm}\beta>0 \\
            & \hspace{6mm} \text{Mean}=\alpha\beta, \text{Var}=\alpha\beta^{2}
    \end{split}\\
    \begin{split}
        (d)& \hspace{2mm} \text{Exponential} \hspace{2mm} f(x|\beta) = \frac{1}{\beta}e^{-x/\beta}, \hspace{2mm} x \in \mathbb{R}_{+}, \beta>0 \\
            & \hspace{6mm} \text{Mean}=\beta, \text{Var}=\beta^{2}
    \end{split}\\
    \begin{split}
        (d)& \hspace{2mm} \text{Chi-Squared} \hspace{2mm} f(x|p) = \frac{x^{(p/2)-1}e^{-x/2}}{\Gamma(p/2)2^{p/2}}, \hspace{2mm} x \in \mathbb{R}_{+}, p=1,2,3,\ldots \\
            & \hspace{6mm} \text{Mean}=p, \text{Var}=2p
    \end{split}
\end{flalign}

%\subsection*{Miscellaneous}
%To include: MGFs and/or CDFs and/or moments for above distributions. More distributions.
%Misc. useful things: L'H\^{o}pital's rule, Taylor approximation, definitions of e, Leibnitz's Rule.

%------------------------
% BEGINNING CHEATSHEET #2
%------------------------

\subsection*{Probability Inequalities}
\textbf{Thm 1 (Gaussian Tail Inequality):}
Let $X \sim \mathcal{N}(0,1)$. Then
\begin{equation}
    \mathbb{P}(|X| > \epsilon) \leq \frac{2}{\epsilon}e^{-\epsilon^{2}/2}
\end{equation}
Additionally:
\begin{equation}
\mathbb{P}(|\overline{X}_{n}| > \epsilon) \leq \frac{1}{\sqrt{n}\epsilon}e^{-n\epsilon^{2}/2}
\end{equation}
    
\textbf{Thm 2 (Markov Inequality):}
Let X be a non-negative random variable s.t. $\mathbb{E}(X)$ exists. Then $\forall$ $t>0$
\begin{equation}
    \mathbb{P}(X>t) \leq \frac{\mathbb{E}(X)}{t}
\end{equation}

\textbf{Thm 3 (Chebyshev's Inequality):}
Let $\mu = \mathbb{E}(X)$ and $\sigma^{2} = \text{Var}(X)$. Then
\begin{gather}
    \mathbb{P}(|X-\mu| \geq t) \leq \frac{\sigma^{2}}{t^{2}} \\
    \mathbb{P}(|(X-\mu)/\sigma| \geq t) \leq \frac{1}{t^{2}}
\end{gather}

\textbf{Lemma 4:}
Let $\mathbb{E}(X) = 0$ and $a \leq X \leq b$. Then
\begin{equation}
    \mathbb{E}(e^{tX}) \leq e^{t^{2}(b-a)^{2}/8}
\end{equation}

\textbf{Lemma 5:}
Let $X$ be any random variable. Then
\begin{equation}
    \mathbb{P}(X>\epsilon) \leq \inf_{t \geq 0} e^{-t\epsilon} \mathbb{E}(e^{tX})
    %\mathbb{P}(X>\epsilon) \leq \underset{t \geq 0}{\text{inf}} e^{-t\epsilon} \mathbb{E}(e^{tX})
\end{equation}

\textbf{Thm 6 (Hoeffding's Inequality):}
$X_{1},\ldots,X_{n}$ iid, $\mathbb{E}(X_{i}) = \mu$, $a \leq X_{i} \leq b$. Then $\forall \epsilon >0$
\begin{equation}
    \mathbb{P}(|\overline{X} - \mu| \geq \epsilon) \leq 2e^{-2n\epsilon^{2}/(b-a)^{2}}
\end{equation}

\textbf{Thm 9 (McDiarmid):} $X_{1},\ldots,X_{n}$ indep't. If\\
$\sup_{x_{1},\ldots,x_{n},x'_{i}} \left| g(x_{1},\ldots,x_{n}) - g_{i}^{*}(x_{1},\ldots,x_{n}) \right|$ $\leq c_{i}$ $\forall i$, $\implies$
\begin{equation}
    \mathbb{P} \left(g(X_{1},\ldots,X_{n})-\mathbb{E}(g(X_{1},\ldots,X_{n})) \geq \epsilon \right) \leq e^{-2\epsilon^{2}/\sum_{i}c_{i}^{2}}
\end{equation}
where $g_{i}^{*} = g$ with $x_{i}$ replaced by $x'_{i}$.

%% Did not yet include on sheet:
%\textbf{Thm 12 (Cauchy-Schwartz inequality):}

%\textbf{Thm 13 (Jensen's inequality):}

%\textbf{Thm 18:}


\subsection*{Shattering}
$F$ a finite set, $|F| = n$, and $G \subset F$. $\mathcal{A}$ is a class of sets.\\
$\mathcal{A}$ \textbf{picks out} $G$ if $\exists A \in \mathcal{A}$ s.t. $A \cap F = G$.\\
Let $S(\mathcal{A},F)$ $=$ $\#\{G \subset F \text{ picked out by } \mathcal{A}\}$ $\leq 2^{n}$.\\
$F$ is \textbf{shattered} by $\mathcal{A}$ if $S(\mathcal{A},F) = 2^{n}$ (ie if $\mathcal{A}$ picks out all $G \subset F$).\\
Let $\mathcal{F}_{n}$ be all finite sets with $n$ elements.\\
The \textbf{shatter coefficient} $s_{n}(\mathcal{A}) = \sup_{F \in \mathcal{F}_{n}} s(\mathcal{A},F) \leq 2^{n}$.\\
The \textbf{VC dimension} $d(\mathcal{A}) =$ the largest $n$ s.t. $s_{n}(\mathcal{A}) = 2^{n}$.\\
\textbf{Thm 5:} $\forall \epsilon>0$, $\mathbb{P}(\sup_{A \in \mathcal{A}} |P_{n}(A) - P(A)| > \epsilon) \leq 8 s_{n}(\mathcal{A})e^{-n\epsilon^{2}/32}$

\subsection*{Random Samples}
For $X_{1},\ldots,X_{n} \sim F$ a \textbf{statistic} is any $T = g(X_{1},\ldots,X_{n})$.\\
E.g. $\overline{X}_{n}$, $S_{n}^{2} = \sum_{i}(X_{i}-\overline{X}_{n})^{2} / (n-1)$, $\left(X_{(1)},\ldots,X_{(n)}\right)$\\
\textbf{Notes:} $\mathbb{E}(\overline{X}_{n}) = \mathbb{E}(X_{i})$, $\text{Var}(\overline{X}_{n}) = \text{Var}(X_{i})/n$, $\mathbb{E}(S_{n}^{2}) = \text{Var}(X_{i})$\\
    \newlinetab $X_{1,\ldots,n}$ $\sim$ $\text{Bern}(p)$ $\implies$ $\sum_{i} X_{i}$ $\sim$ $\text{Bin}(n,p)$ \\
    \newlinetab $X_{1,\ldots,n} \sim \text{Exp}(\beta)$ $\implies$ $\sum_{i}X_{i}$ $\sim$ $\Gamma(n,\beta)$ \\
    \newlinetab $X_{1,\ldots,n} \sim \mathcal{N}(0,1)$ $\implies$ $\sum_{i}X_{i}^{2} \sim \chi_{n}$.

\textbf{Thm. 1}: $X_{1},\ldots,X_{n} \sim \mathcal{N}(\mu,\sigma^{2})$ $\implies$  $\overline{X}_{n} \sim \mathcal{N}(\mu, \sigma^{2}/n)$.

\subsection*{Convergence}
$X,X_{1},X_{2},\ldots$ random variables.\\
(1) $X_{n}$ converges \textbf{almost surely} $X_{n} \xrightarrow{a.s.} X$ if $\forall \epsilon>0$
\begin{equation}
    \mathbb{P}(\lim_{n\rightarrow\infty} |X_{n}-X| < \epsilon) = 1
\end{equation}
(2) $X_{n}$ converges \textbf{in probability} $X_{n} \xrightarrow{p} X$ if $\forall \epsilon>0$
\begin{equation}
    \lim_{n\rightarrow\infty} \mathbb{P}(|X_{n}-X| > \epsilon) = 0 
\end{equation}
(3) $X_{n}$ converges \textbf{in quadratic mean} $X_{n} \xrightarrow{qm} X$ if
\begin{equation}
    \lim_{n\rightarrow\infty} \mathbb{E}[(X_{n}-X)^{2}] = 0
\end{equation}
(4) $X_{n}$ converges \textbf{in distribution} $X_{n} \rightsquigarrow X$ if
\begin{equation}
    \lim_{n\rightarrow\infty} F_{X_{n}}(t) = F_{X}(t)
\end{equation}
$\forall t$ on which $F_{X}$ is continuous.\\

\textbf{Thm 7:} Conv. a.s. and in q.m. imply conv. in prob. All three\\
    \newlinetab imply conv. in distribution. Conv. in distribution to a point-\\
    \newlinetab mass also implies conv. in prob.\\
%Ex from class: Showed conv. in prob $\not\implies$ conv. a.s.. Showed conv. in prob $\not\implies$ conv. in q.m.. Showed conv. in distro $\not\implies$ conv. in prob.

\textbf{Thm 10a:} $X$,$X_{n}$,$Y$,$Y_{n}$ random variables. Then
\begin{flalign}
    (a)& \hspace{2mm} X_{n} \xrightarrow{p} X, Y_{n} \xrightarrow{p} Y \implies X_{n} + Y_{n} \xrightarrow{p} X + Y \\
    (b)& \hspace{2mm} X_{n} \xrightarrow{p} X, Y_{n} \xrightarrow{p} Y \implies X_{n}Y_{n} \xrightarrow{p} XY \\
    (c)& \hspace{2mm} X_{n} \xrightarrow{qm} X, Y_{n} \xrightarrow{qm} Y \implies X_{n} + Y_{n} \xrightarrow{qm} X + Y
\end{flalign}

\textbf{Thm 10b (Slutzky's Thm):} $X$,$X_{n}$,$Y_{n}$ random variables. Then
\begin{flalign}
    (a)& \hspace{2mm} X_{n} \rightsquigarrow X, Y_{n} \rightsquigarrow c \implies X_{n} + Y_{n} \rightsquigarrow X + c \\
    (b)& \hspace{2mm} X_{n} \rightsquigarrow X, Y_{n} \rightsquigarrow c \implies X_{n}Y_{n} \rightsquigarrow cX
\end{flalign}

\textbf{Thm 12 (Law of Large Numbers):} $X_{1},\ldots,X_{n}$ iid, $\mathbb{E}(X_{i})=\mu$\\
 \newlinetab $\implies$ $\overline{X}_{n} \xrightarrow{\text{qm}} \mu$.

\textbf{Thm 14 (CLT):} $X_{1},\ldots,X_{n}$ iid, $\mathbb{E}(X_{i})=\mu$ $\text{Var}(X_{i}) = \sigma^{2}$\\
    \newlinetab $\implies$ $\sqrt{n}(\overline{X}_{n}-\mu)/\sigma \rightsquigarrow \mathcal{N}(0,1)$\\
    \newlinetab $\implies$ $\sqrt{n}(\overline{X}_{n}-\mu) \rightsquigarrow \mathcal{N}(0,\sigma^{2})$\\
    \newlinetab $\implies$ $\overline{X}_{n} \approx \mathcal{N}(\mu,\sigma^{2}/n)$\\
    \newlinetab $\implies$ $\sqrt{n}(\overline{X}_{n}-\mu)/S_{n}\rightsquigarrow \mathcal{N}(0,1)$

\textbf{Thm 18 (delta method):} If $\sqrt{n}(Y_{n}-\mu)/\sigma \rightsquigarrow \mathcal{N}(0,1)$, $g'(\mu) \neq 0$ \\
    \newlinetab $\implies$ $\sqrt{n}(g(Y_{n})-g(\mu))/|g'(\mu)|\sigma \rightsquigarrow \mathcal{N}(0,1)$\\
    \newlinetab ie $Y_{n} \approx \mathcal{N}(\mu,\sigma^{2}/n)$ $\implies$ $g(Y_{n}) \approx \mathcal{N}(g(\mu),g'(\mu)^{2}\sigma^{2}/n)$

\textbf{Thm 18b (2nd order delta method):}

\subsection*{Sufficiency}
If $X_{1},\ldots,X_{n} \sim p(x;\theta)$, $T$ \textbf{sufficient} for $\theta$ if $p(x^{n}|t;\theta)$ $=$ $p(x^{n}|t)$.
\textbf{Thm 9 (factorization):} for $X^{n} \sim p(x;\theta)$, $T(X^{n})$ sufficient for $\theta$ \\
    \newlinetab if the joint probability can be factorized as
\begin{equation}
    p(x^{n};\theta) = h(x^{n}) \times g(t;\theta)
\end{equation}
$T$ is a \textbf{minimal sufficient statistic (MSS)} if $T$ is sufficient and \\
    \newlinetab $T = g(U)$ for all other sufficient stats $U$.\\
\textbf{Thm 15:} $T$ is a MSS if:
\begin{equation}
    \frac{p(y^{n};\theta)}{p(x^{n};\theta)} \text{ is constant in $\theta$ } \iff T(y^{n}) = T(x^{n})
\end{equation}

\subsection*{Parametric Point Estimation}
\textbf{Method of Moments:} Define equations\\
    \newlinetab (a) \hspace{3mm} $(\sum_{i} X_{i})/n = \mathbb{E}_{\hat{\theta}}(X_{i})$\\
    \newlinetab (b) \hspace{3mm} $(\sum_{i} X_{i}^{2})/n = \mathbb{E}_{\hat{\theta}}(X_{i}^{2})$\\
    \newlinetab (c) \hspace{3mm} $\ldots$ \\
    \newlinetab and solve for $\hat{\theta}$.

\textbf{Maximum Likelihood (MLE):} The MLE is
\begin{equation}
\hat{\theta} = \text{arg}\max_{\theta} L(\theta) = \text{arg}\max_{\theta} l(\theta)
\end{equation}
Often suffices to solve for $\theta$ in $\frac{\partial l(\theta)}{\partial \theta} = 0$.\\
The MLE is \textbf{equivariant} $\implies$ if $\eta = g(\theta)$ then $\hat{\eta} = g(\hat{\theta})$. \\
\textbf{Bayes Estimation:} For prior $\pi(\theta)$, choose 
\begin{equation}
    \hat{\theta} = \mathbb{E}(\theta|x^{n}) = \int \theta \pi(\theta|x^{n}) d\pi
\end{equation}
\textbf{Mean Squared Error (MSE):} The MSE is
\begin{equation}
    \text{MSE} = \mathbb{E}(\hat{\theta} - \theta)^{2} = \int (\hat{\theta}-\theta)^{2} p(x^{n};\theta)dx^{n} = \text{bias}({\hat{\theta}})^{2} + Var(\hat{\theta})
\end{equation}
Defs: $\text{\textbf{bias}}(\hat{\theta})$ $=$ $\mathbb{E}(\hat{\theta}) - \theta$. We say $\hat{\theta}$ is \textbf{consistent} if $\hat{\theta} = \hat{\theta}_{n} \xrightarrow{p} \theta$. The \textbf{standard error} of $\hat{\theta}$, $\text{se}(\hat{\theta})$, is the standard deviation of $\hat{\theta}$.\\

\subsection*{Risks and Estimators}
$L(\theta,\hat{\theta})$ is the \textbf{loss} of an estimator $\hat{\theta} = \hat{\theta}(x^{n})$ for $x^{n} \sim p(x^{n};\theta)$.\\
The \textbf{risk} of this $\hat{\theta}$ is
\begin{equation}
    R(\theta,\hat{\theta}) = \mathbb{E}[L(\theta,\hat{\theta})] = \int L(\theta,\hat{\theta}) p(x^{n};\theta) dx^{n}
\end{equation}

When $L(\theta,\hat{\theta}) = (\theta-\hat{\theta})^{2}$, the risk is the MSE.

The \textbf{max risk} of $\hat{\theta}$ over a set $\theta \in \Theta$ is
\begin{equation}
    \overline{R}(\hat{\theta}) = \sup_{\theta \in \Theta} R(\theta,\hat{\theta})
\end{equation}

%The \textbf{minimax risk} is
%\begin{equation}
    %R_{n} = \inf_{\hat{\theta}} \sup_{\theta \in \Theta} R(\theta,\hat{\theta})
%\end{equation}

The \textbf{minimax estimator} is
\begin{equation}
    \hat{\theta} = \text{arg}\inf_{\hat{\theta}} \overline{R}(\hat{\theta})
\end{equation}

The \textbf{Bayes risk} of $\hat{\theta}$ given a prior $\pi(\theta)$ is
\begin{equation}
    B_{\pi}(\hat{\theta}) = \int R(\theta,\hat{\theta}) \pi(\theta) d\theta
\end{equation}

The \textbf{posterior risk} of $\hat{\theta}$ given a prior $\pi(\theta)$ is
\begin{equation}
    r(\hat{\theta}|x^{n}) = \int L(\theta,\hat{\theta}) \pi(\theta|x^{n}) d\theta    
\end{equation}
\newlinetab where $\pi(\theta|x^{n}) = \frac{\mathbb{P}(x^{n};\theta)\pi(\theta)}{m(x^{n})}$ is the posterior over $\theta$.

The \textbf{Bayes estimator} is
\begin{equation}
    \hat{\theta} = \text{arg}\inf_{\hat{\theta}} B_{\pi}(\hat{\theta}) = \text{arg}\inf_{\hat{\theta}} r(\hat{\theta}|x^{n})
\end{equation}
which equals the posterior mean $\mathbb{E}(\theta|x^{n})$ when $L(\theta,\hat{\theta}) = (\theta-\hat{\theta})^{2}$,\\
    \newlinetab the posterior median when $L(\theta,\hat{\theta}) = |\theta-\hat{\theta}|$, and the posterior\\
    \newlinetab mode when $L(\theta,\hat{\theta}) = \mathbb{I}[\theta \neq \hat{\theta}]$.

\textbf{Thm 10:} If $\hat{\theta}$ is a Bayes estimator for some prior $\pi$ and $R(\theta,\hat{\theta})$ is\\
    \newlinetab constant, then $\hat{\theta}$ is a minimax estimator.\\
\textbf{Note:} The MLE is approximately minimax (as n increases, if\\
    \newlinetab the dimension of the parameter is fixed).


%------------------------
% BEGINNING CHEATSHEET #3
%------------------------

\subsection*{Aymptotic (Large Sample) Theory}
A random sequence $A_{n}$ is:
\begin{flalign}
    &(a) \hspace{2mm} o_{p}(1) \text{ if } A_{n} \xrightarrow{p} 0\\
    &(b) \hspace{2mm} o_{p}(B_{n}) \text{ if } A_{n}/B_{n} \xrightarrow{p} 0\\
    &(c) \hspace{2mm} O_{p}(1) \text{ if } \forall \epsilon>0, \exists M :
        \lim_{n\rightarrow \infty} \mathbb{P}(|A_{n}|>M)<\epsilon \\
    &(d) \hspace{2mm} O_{p}(B_{n}) \text{ if } A_{n}/B_{n} = O_{p}(1)
\end{flalign}
If $Y_{n} \rightsquigarrow Y \implies Y_{n}=O_{p}(1)$\\
If $\sqrt{n}(Y_{n}-c)\rightsquigarrow Y$ $\implies$ $Y_{n}=O_{p}(1/\sqrt{n})$

\subsubsection*{Distances Between Distributions}
For distributions $P$ and $Q$ with pdfs $p$ and $q$:
\begin{flalign}
    &(a) \hspace{2mm} V(P,Q) = \sup_{A} |P(A)-Q(A)| \hspace{3mm} \text{\textbf{total variation} distance}\\
    &(b) \hspace{2mm} K(P,Q) = \int p \text{log}(p/q) \hspace{3mm} \text{\textbf{Kullback-Leibler} divergence}\\
    &(c) \hspace{2mm} d_{2}(P,Q) = \int (p-q)^{2} \hspace{3mm} \mathbf{L_{2}} \text{ distance}
\end{flalign}
A model is \textbf{identifiable} if: $\theta_{1} \neq \theta_{2}$ $\implies$ $K(\theta_{1},\theta_{2})>0$.

\subsubsection*{Consistency}
$\hat{\theta}_{n} = T(X^{n})$ is \textbf{consistent} for $\theta$ if $\hat{\theta}_{n} \xrightarrow{p} \theta$
    (ie if $\hat{\theta}_{n} - \theta = o_{p}(1)$).\\
To show consistency, can show: $\text{Bias}^{2}(\hat{\theta}_{n}) + \text{Var}(\hat{\theta}_{n}) \rightarrow 0$.\\
The MLE is consistent under regularity conditions.\\
MLE not consistent when number of params (or support?) grows.

\subsubsection*{Score and Fisher Information}
The \textbf{score function} is $S(\theta) = \frac{\partial}{\partial\theta} l(\theta) = \frac{\partial}{\partial\theta} \sum_{i=1}^{n} \text{log} \hspace{1pt} p(x_{i}|\theta)$.\\
The \textbf{Fisher information} is defined as
\begin{equation}
    I_{n}(\theta) = \mathbb{E}_{\theta} \left[ S(\theta)^{2} \right] = \text{Var}_{\theta} \left[ S(\theta) \right] 
        = -\mathbb{E}_{\theta} \left[ \frac{\partial^{2}}{\partial\theta^{2}} l(\theta) \right]
        %= -\mathbb{E}_{\theta} \left[ \frac{\partial}{\partial\theta} S(\theta) \right]
\end{equation}
    \newlinetab and $I_{n}(\theta) = -n\mathbb{E} \left[ \frac{\partial^{2}}{\partial\theta^{2}} \text{log}\hspace{1pt} p(X_{1};\theta) \right] = nI_{1}(\theta)$.\\
The \textbf{observed information} $\hat{I}_{n}(\theta) = -\sum_{i}\frac{\partial^{2}}{\partial\theta^{2}} \text{log}\hspace{1pt}p(X_{i};\theta)$.\\
    \newlinetab Vector case: $S(\theta) = \left[ \frac{\partial l(\theta)}{\partial \theta_{i}} \right]_{i=1,\ldots,K}$ \hspace{1mm}
        $I_{ij} = -\mathbb{E}_{\theta}\left[\frac{\partial^{2} l(\theta)}{\partial\theta_{i}\partial\theta_{j}}\right]_{i,j=1,\ldots,K}$

\subsubsection*{Efficiency and Robustness}
For an estimator $\hat{\theta}_{n}(X^{n})$ of $\theta$, where $X^{n} \stackrel{\text{iid}}{\sim} p(x|\theta)$:\\
If $\sqrt{n}(\hat{\theta}_{n} - \theta) \rightsquigarrow \mathcal{N}(0,v^{2})$, then $v^2$ is the \textbf{asymptotic-Var($\hat{\theta}_{n}$)}.\\
    \newlinetab E.g. for $\hat{\theta}_{n} = \overline{X}_{n}$:
        \hspace{1pt} $v^{2} = \sigma^{2} = \text{Var}(X_{i}) = \lim_{n \rightarrow \infty} n\text{Var}(\overline{X}_{n})$.\\
    \newlinetab In general, asymptotic-Var($\hat{\theta}_{n}$) $v^{2}$ $\neq$ $\lim_{n \rightarrow \infty} n\text{Var}(\hat{\theta}_{n})$.\\
    \newlinetab Therefore: $\text{Var}(\hat{\theta}_{n}) = (\text{se})^{2} \approx v^{2}/n$.\\
For param $\tau(\theta)$, $v(\theta) = \frac{|\tau'(\theta)|^{2}}{I_{1}(\theta)}$ is the \textbf{Cramer-Rao lower bound}.\\
    \newlinetab for most estimators $v(\theta) \leq v^{2}$.\\
If $\sqrt{n}(\hat{\theta}_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,v(\theta))$ (ie if $v^{2} = v(\theta)$) $\implies \hat{\theta}_{n}$ \textbf{efficient}.\\
    \newlinetab usually, $\sqrt{n}(\tau(\hat{\theta}_{\text{mle}}) - \tau(\theta)) \rightsquigarrow \mathcal{N}(0,v(\theta))$ $\implies$ MLE efficient.\\
The \textbf{standard error} of \textbf{efficient} $\hat{\theta}_{n}$ is $se = \sqrt{\text{Var}(\hat{\theta}_{n})} \approx \sqrt{\frac{1}{I_{n}(\theta)}}$.\\
The \textbf{estimated standard error} of \textbf{efficient} $\hat{\theta}_{n}$ is $\hat{se} \approx \sqrt{\frac{1}{I_{n}(\hat{\theta}_{n})}}$.\\
    \newlinetab For efficient $\hat{\theta}_{n}$, $\hat{\tau} = \tau(\hat{\theta}_{n})$, $se \approx \sqrt{\frac{|\tau'(\theta)|^{2}}{I_{n}(\theta)}}$,
        and $\hat{se} \approx \sqrt{\frac{|\tau'(\hat{\theta}_{n})|^{2}}{I_{n}(\hat{\theta}_{n})}}$.\\
In general, \textbf{asymptotic normality} is when:\\
    \newlinetab $\frac{\hat{\theta}_{n} - \mathbb{E}(\hat{\theta}_{n})}{\sqrt{\text{Var}(\hat{\theta}_{n})}} \rightsquigarrow \mathcal{N}(0,1)$
        $\implies$ $\hat{\theta}_{n} \rightsquigarrow \mathcal{N}(\mathbb{E}(\hat{\theta}_{n}), \text{Var}(\hat{\theta}_{n}))$. \\
If $\sqrt{n}(W_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,\sigma^{2}_{W})$ and
    $\sqrt{n}(V_{n}-\tau(\theta)) \rightsquigarrow \mathcal{N}(0,\sigma^{2}_{V})$ \\
    $\text{ }\hspace{2mm}$$\implies$ \textbf{asymptotic relative efficiency} $\text{ARE}(V_{n},W_{n}) = \sigma^{2}_{W} / \sigma^{2}_{V}$.\\
Often there is a tradeoff between efficiency and robustness. (?)

\subsection*{Hypothesis Testing}
\textbf{Null hypothesis} $H_{0}: \theta \in \Theta_{0}$, \textbf{alternative} $H_{1}: \theta \in \Theta_{1}$.\\
\textbf{Type I error}: If $H_{0}$ true but we reject $H_{0}$.\\
To construct a test:
\begin{equation}
    \begin{split}
        &1. \text{ Choose a test statistic } W = W(X_{1},\ldots,X_{n})\\
        &2. \text{ Choose a rejection region } R\\
        &3. \text{ If } W\in R, \text{ reject } H_{0} \text{ otherwise retain } H_{0}
    \end{split}
\end{equation}
The \textbf{power function} $\beta(\theta) = \mathbb{P}_{\theta}(W \in R)$ for a rejection region $R$.\\
Want \textbf{level-$\mathbf{\alpha}$} test ($\sup_{\theta \in \Theta_{0}} \beta(\theta) \leq \alpha$) that maximizes $\beta(\theta\in\Theta_{1})$.\\
A level-$\alpha$ test with power fn $\beta$ is \textbf{uniformly most powerful} if:
    \newlinetab$\beta(\theta) \geq \beta'(\theta)$ $\forall \theta \in \Theta_{1}$ $\forall \beta'\neq\beta$.

\subsubsection*{Neyman-Pearson Test}
For simple $H_{0}: \theta=\theta_{0}$ and $H_{1}: \theta=\theta_{1}$, reject $H_{0}$ if $\frac{L(\theta_{1})}{L(\theta_{0})} > k$.
    \newlinetab where $k$ chosen s.t. $\mathbb{P}(\frac{L(\theta_{1})}{L(\theta_{0})} > k) = \alpha$.

\subsubsection*{Wald Test}
For $H_{0}: \theta=\theta_{0}$ and $H_{1}: \theta\neq\theta_{0}$, reject $H_{0}$ if $\left| \frac{\hat{\theta}_{n} - \theta_{0}}{se} \right| > z_{\alpha/2}$.\\
    \newlinetab where $z_{\alpha/2}$ is the inverse standard-normal CDF of $1-\frac{\alpha}{2}$. \\%, ie $\Phi^{-1}(1-\alpha/2)$
    \newlinetab and $\hat{\theta}_{n}$ an estimator s.t. $(\hat{\theta}-\theta)/\text{se} \rightsquigarrow \mathcal{N}(0,1)$ \hspace{2mm} eg: $\hat{\theta}_{\text{mle}}$\\
    \newlinetab and $se = \sqrt{\text{Var}(\hat{\theta}_{n})}$. Can also use $\hat{se} =_{\text{eg.}} \sqrt{S_{n}^{2}/n}$.\\
    \newlinetab and if $\hat{\theta}_{n}$ efficient, can approx: $se \approx \sqrt{\frac{1}{I_{n}(\theta)}}$ or $\hat{se} \approx \sqrt{\frac{1}{I_{n}(\hat{\theta}_{n})}}$.

\subsubsection*{Likelihood Ratio Test}
For $H_{0}: \theta\in\Theta_{0}$ and $H_{1}: \theta\notin\Theta_{0}$, reject $H_{0}$ if $\lambda(x^{n}) = \frac{L(\hat{\theta}_{0})}{L(\hat{\theta})} \leq c$.
    \newlinetab where $L(\hat{\theta}_{0}) = \sup_{\theta\in\Theta_{0}}L(\theta)$ and $L(\hat{\theta}) = \sup_{\theta\in\Theta}L(\theta)$.\\
    \newlinetab and $c$ chosen s.t. $\mathbb{P}(\lambda(x^{n}) \leq c) = \alpha$.\\
    \newlinetab \textbf{Thm:} under $H_{0} : \theta=\theta_{0}$ $\implies$ $W_{n} = -2\text{log}\lambda(X^{n}) \rightsquigarrow \chi_{1}^{2}$\\
    \newlinetab\newlinetab $\implies$ reject $H_{0}$ if $W_{n}>\chi_{1,\alpha}^{2}$.\\ %\lim_{n\rightarrow\infty} \mathbb{P}_{\theta_{0}}(W_{n} > \chi_{1,\alpha}^{2}) = \alpha$.\\
        \newlinetab\newlinetab Also: for $\theta=(\theta_{1},\ldots,\theta_{k})$, if $H_{0}$ fixes some of the parameters\\
        \newlinetab\newlinetab $\implies$ $-2\text{log}\lambda(X^{n}) \rightsquigarrow \chi_{\nu}^{2}$, where $\nu = \text{dim}(\Theta) - \text{dim}(\Theta_{0})$.

\subsubsection*{P-Values}
The \textbf{p-value} $p(x^{n})$ is the smallest $\alpha$-level s.t. we reject $H_{0}$.\\
\textbf{Thm:} For a test of the form: reject $H_{0}$ when $W(x^{n})>c$,\\
    $\text{ }\hspace{1mm}$ $\implies$ $p(x^{n}) = \underset{\theta\in\Theta_{0}}{\sup} \mathbb{P}_{\theta}(W(X^{n}) \geq W(x^{n}))$
    $=$ $\underset{\theta\in\Theta_{0}}{\sup} [1 - F(W(x^{n})|\theta)]$.\\
\textbf{Thm:} Under $H_{0}:\theta=\theta_{0}$,\hspace{2mm}$p(x^{n}) \sim \text{Unif}(0,1)$.

\subsubsection*{Permutation Test}
$X^{n} \sim F$, $Y^{m} \sim G$, $H_{0}:F=G$, $H_{1}:F \neq G$\\
Let $Z=(X^{n},Y^{m})$ and $L=(1,\ldots,1,2,\ldots,2)$.\\
Let $W = g(L,Z) = |(\text{ave of 1 labeled pts}) - (\text{ave of 2 labeled pts})|$.
Let $p = \frac{1}{N!}\sum_{\pi} \mathbb{I}\left( g(L_{\pi},Z) > g(L,Z) \right)$ $\implies$ reject $H_{0}$ when $p<\alpha$.

\subsection*{Confidence Intervals}
We want a $1-\alpha$ \textbf{confidence interval} $C_{n} = [L(X^{n}),U(X^{n})]$ s.t.\\
    \newlinetab$\mathbb{P}_{\theta} \left( L(X^{n}) \leq \theta \leq U(X^{n}) \right)$ $\geq$ $1-\alpha$, \hspace{1mm} $\forall \theta\in\Theta$.\\
Generally, a $1-\alpha$ \textbf{confidence set} $C_{n}$ is a random set $C_{n} \subset \Theta$ s.t.\\
    \newlinetab $\inf_{\theta\in\Theta}\mathbb{P}_{\theta}\left( \theta \in C_{n}(X^{n}) \right) \geq 1-\alpha$.\\

\subsubsection*{Using Probability Inequalities}
Prob inequalities give (for eg.) $\mathbb{P}(|\hat{\theta}_{n} - \theta| > \epsilon) \leq g(\exp^{-f(\epsilon)}) \underset{\text{set to}}{=} \alpha$.\\
    \newlinetab solving for $\epsilon$ gives $\epsilon = \tilde{f}(\alpha)$ $\implies$ $\mathbb{P} \left( |\hat{\theta}_{n} - \theta| > \tilde{f}(\alpha) \right) \leq \alpha$\\
    \newlinetab $\implies C_{n} = \left( \hat{\theta}-\tilde{f}(\alpha), \hat{\theta}+\tilde{f}(\alpha) \right)$.\\

\subsubsection*{Inverting a Test}
In level-$\alpha$ tests $\mathbb{P}_{\theta_{0}}(T(x^{n}) \in R) \leq \alpha$ $\Rightarrow$ let $C_{n} = \{ \theta : T(x^{n}) \in A(\theta_{0}) \}$.
    $\text{ }\hspace{1mm}$where $A(\theta_{0}) = \{ T(x^{n}) \notin R \hspace{1mm} | \hspace{1mm} \theta = \theta_{0} \}$ (ie the accept region if $\theta = \theta_{0}$).\\
    For Wald: $C_{n} = \hat{\theta}_{n} \pm (z_{\alpha/2} \times se) = \hat{\theta}_{n} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$.\\
For LRT: $C_{n} = \{ \theta : \frac{L(\theta)}{L(\hat{\theta})} > c \}$ (for test where reject $H_{0}$ if $\frac{L(\theta_{0})}{L(\hat{\theta})} \leq c$).

\subsubsection*{Pivots}
$Q(X^{n},\theta)$ a \textbf{pivot} if the distribution of $Q$ does not depend on $\theta$.\\
Find $a$, $b$ s.t. $\mathbb{P}_{\theta}(a \leq Q(X^{n},\theta) \leq b) \geq 1-\alpha$, $\forall \theta$.\\
\newlinetab $\implies$ $C_{n} = \{ \theta : a \leq Q(X^{n},\theta) \leq b) \geq 1-\alpha \}$.


%----------------------------------
% BEGINNING ONLY-FOR-FINAL MATERIAL 
%----------------------------------

\subsection*{Nonparametric Inference}
The \textbf{empirical CDF} is: $\hat{F}_{n}(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x)$ \\
\textbf{Thm (DKW):} $\forall \epsilon >0, \mathbb{P}(\sup_{x} |\hat{F}(x) - F(x)| > \epsilon) \leq 2e^{-2n\epsilon^{2}}$\\
The \textbf{kernel density estimator} is: $\hat{p}_{n}(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h} K \left( \frac{x-X_{i}}{h} \right)$\\
    \newlinetab where K a symmetric zero-mean density, and bandwidth $h>0$.
\textbf{Thm:} The risk $R = \mathbb{E}(\mathcal{L}(p,\hat{p})) = \int (\text{b}^{2}(x) + \text{Var}(x))dx = \frac{a}{n^{4/5}}$\\
    \newlinetab for some $a$, where $\mathcal{L}(p,\hat{p}) = \int (p(x) - \hat{p}(x))^{2}dx$, and \\
    \newlinetab $\text{b}^{2}(x) = \mathbb{E}(\hat{p}(x)) - p(x)$. And this is minimax.\\
A \textbf{statistical functional} $T(F)$ is any function of the CDF.\\
A \textbf{plug-in estimator} of $\theta=T(F)$ is: $\hat{\theta}_{n} = T(\hat{F}_{n})$.\\
Often, $\hat{\theta}_{n} \approx \mathcal{N}(T(F),\hat{\text{se}}^{2})$, where $\hat{\text{se}}$ is estimate of $\sqrt{\text{Var}\left( T(\hat{F}_{n}) \right)}$.

\subsubsection*{Bootstrap}
The \textbf{bootstrap} is a nonparametric way to find standard errors\\
    \newlinetab and confidence intervals of estimators of statistical functionals:
(1) Draw $X_{1}^{*},\ldots,X_{n}^{*} \sim \hat{F}_{n}$ (by drawing $X_{i}^{*} \sim \{ X_{1},\ldots,X_{n} \}$ unif).\\
(2) Compute $T_{n}^{*} = g(X_{1}^{*},\ldots,X_{n}^{*})$\\
(3) Do (1) and (2) $B$ times to get $T_{n,1}^{*},\ldots,T_{n,B}^{*}$\\
(4) Let $v_{\text{boot}} = \frac{1}{B} \sum_{b=1}^{B} \left( T_{n,b}^{*} - \frac{1}{B}\sum_{r=1}^{B}T_{n,r}^{*} \right)^{2}$\\
Then: $v_{\text{boot}} \xrightarrow{a.s.} \text{Var}_{\hat{F}_{n}}(T_{n})$ as $B \rightarrow \infty$ and $\hat{\text{se}}(T_{N}) = \sqrt{v_{\text{boot}}}$

\subsection*{Bayesian Inference}
Frequentists: probability is long-run frequencies. Procedures are\\
    \newlinetab random but parameters are fixed, unknown quantities.\\
Bayesians: probability is a measure of subjective degree of belief. \\
    \newlinetab Everything is random, including parameters.\\
Using Bayes Thm $\nRightarrow$ Bayesian inference.\\
For $X_{1},\ldots,X_{n} \sim p(x|\theta)$, and prior $\pi(\theta)$, \textbf{Bayes Thm} gives:\\
    \newlinetab $\pi(\theta|X^{n}) = \frac{p(X^{n}|\theta)\pi(\theta)}{m(X^{n})} = \frac{p(X^{n}|\theta)\pi(\theta)}{\int p(X^{n}|\theta)\pi(\theta)d\theta}$\\

\end{document}
